{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Models for Toy Problem 5\n",
    "\n",
    "How do we train a classifier to count the non-zero values in a sequence of integers?\n",
    "\n",
    "We have the data ... having already generated data points consisting of features that consist of a sequence of $1$s and $0$ and an output label which is the count of the non-zero features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights input & hidden to hidden => Parameter containing:\n",
      " 0.6588  0.5137  0.2802  0.3886\n",
      " 0.2036  0.6493  0.2917  0.8452\n",
      " 0.1676  0.8665  0.3543  0.9987\n",
      " 0.1394  0.8172  0.3156  0.1504\n",
      " 0.1451  0.5167  0.0110  0.1137\n",
      "[torch.FloatTensor of size 5x4]\n",
      "\n",
      "Bias to hidden => Parameter containing:\n",
      " 0.6514  0.4509  0.4071  0.6924\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "Weights hidden to output categories => Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.7931  0.6467  0.1943  0.4167  0.6571  0.1315  0.7598  0.3914  0.2626  0.7024\n",
      " 0.0998  0.8733  0.9745  0.2042  0.5290  0.1556  0.3832  0.8666  0.3726  0.3792\n",
      " 0.3009  0.1186  0.2703  0.5419  0.7609  0.3759  0.2803  0.9337  0.6753  0.6684\n",
      " 0.3307  0.0428  0.7025  0.7086  0.6252  0.1413  0.1440  0.0387  0.9647  0.4536\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0725  0.8530  0.0865  0.8971  0.6784  0.2744  0.1088  0.8517  0.0380  0.7214\n",
      " 0.3467  0.1338  0.1561  0.9472  0.6366  0.6784  0.0429  0.7818  0.7486  0.0830\n",
      " 0.4039  0.2213  0.8707  0.6026  0.0348  0.9106  0.8854  0.8488  0.0252  0.4866\n",
      " 0.9921  0.7997  0.7675  0.3541  0.3217  0.5219  0.9571  0.1766  0.5532  0.3038\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.7072\n",
      " 0.4256\n",
      " 0.9976\n",
      " 0.9292\n",
      "[torch.FloatTensor of size 4x21]\n",
      "\n",
      "Bias to output categories => Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.6072  0.7289  0.7667  0.1654  0.3130  0.3483  0.7898  0.9230  0.6818  0.3193\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.8155  0.0535  0.0248  0.5091  0.0323  0.7508  0.4639  0.4441  0.4900  0.3508\n",
      "\n",
      "Columns 20 to 20 \n",
      " 0.5505\n",
      "[torch.FloatTensor of size 1x21]\n",
      "\n",
      "The loss is now 3.196824073791504\n",
      "The loss is now 3.05364727973938\n",
      "The loss is now 3.016634225845337\n",
      "The loss is now 3.0045206546783447\n",
      "The loss is now 2.9825844764709473\n",
      "The loss is now 2.910814046859741\n",
      "The loss is now 2.7006547451019287\n",
      "The loss is now 2.534773349761963\n",
      "The loss is now 2.409898281097412\n",
      "The loss is now 2.3135037422180176\n",
      "The loss is now 2.2271618843078613\n",
      "The loss is now 2.1611814498901367\n",
      "The loss is now 2.0913519859313965\n",
      "The loss is now 2.0083813667297363\n",
      "The loss is now 1.9491748809814453\n",
      "The loss is now 1.9182952642440796\n",
      "The loss is now 1.8880809545516968\n",
      "The loss is now 1.8539947271347046\n",
      "The loss is now 1.8308727741241455\n",
      "The loss is now 1.7820661067962646\n",
      "The loss is now 1.7510371208190918\n",
      "The loss is now 1.7250350713729858\n",
      "The loss is now 1.6732442378997803\n",
      "The loss is now 1.684578537940979\n",
      "The loss is now 1.6582471132278442\n",
      "The loss is now 1.6283438205718994\n",
      "The loss is now 1.6086914539337158\n",
      "The loss is now 1.6015870571136475\n",
      "The loss is now 1.579301357269287\n",
      "The loss is now 1.568320631980896\n",
      "The loss is now 1.572335958480835\n",
      "The loss is now 1.5423557758331299\n",
      "The loss is now 1.535569190979004\n",
      "The loss is now 1.5194956064224243\n",
      "The loss is now 1.522611141204834\n",
      "The loss is now 1.5076109170913696\n",
      "The loss is now 1.4957671165466309\n",
      "The loss is now 1.4842190742492676\n",
      "The loss is now 1.5133368968963623\n",
      "The loss is now 1.4557946920394897\n",
      "The loss is now 1.4809014797210693\n",
      "The loss is now 1.4527966976165771\n",
      "The loss is now 1.4567135572433472\n",
      "The loss is now 1.44889497756958\n",
      "The loss is now 1.4139435291290283\n",
      "The loss is now 1.4176946878433228\n",
      "The loss is now 1.410451889038086\n",
      "The loss is now 1.4038554430007935\n",
      "The loss is now 1.4234992265701294\n",
      "The loss is now 1.4032726287841797\n",
      "The loss is now 1.4188216924667358\n",
      "The loss is now 1.3907595872879028\n",
      "The loss is now 1.3644310235977173\n",
      "The loss is now 1.3719207048416138\n",
      "The loss is now 1.3420922756195068\n",
      "The loss is now 1.371037483215332\n",
      "The loss is now 1.3483445644378662\n",
      "The loss is now 1.3436604738235474\n",
      "The loss is now 1.325120210647583\n",
      "The loss is now 1.3103619813919067\n",
      "The loss is now 1.3086973428726196\n",
      "The loss is now 1.2954946756362915\n",
      "The loss is now 1.312682867050171\n",
      "The loss is now 1.323807954788208\n",
      "The loss is now 1.2736965417861938\n",
      "The loss is now 1.2934974431991577\n",
      "The loss is now 1.3114949464797974\n",
      "The loss is now 1.2926604747772217\n",
      "The loss is now 1.2790346145629883\n",
      "The loss is now 1.25187349319458\n",
      "The loss is now 1.2545006275177002\n",
      "The loss is now 1.2649097442626953\n",
      "The loss is now 1.2777514457702637\n",
      "The loss is now 1.2391961812973022\n",
      "The loss is now 1.2391417026519775\n",
      "The loss is now 1.2467387914657593\n",
      "The loss is now 1.2112128734588623\n",
      "The loss is now 1.2089766263961792\n",
      "The loss is now 1.2085824012756348\n",
      "The loss is now 1.2134920358657837\n",
      "The loss is now 1.2108668088912964\n",
      "The loss is now 1.2071810960769653\n",
      "The loss is now 1.192819595336914\n",
      "The loss is now 1.2108662128448486\n",
      "The loss is now 1.2002599239349365\n",
      "The loss is now 1.1948940753936768\n",
      "The loss is now 1.2004178762435913\n",
      "The loss is now 1.1837711334228516\n",
      "The loss is now 1.1871966123580933\n",
      "The loss is now 1.1640952825546265\n",
      "The loss is now 1.170597791671753\n",
      "The loss is now 1.1715288162231445\n",
      "The loss is now 1.1804351806640625\n",
      "The loss is now 1.166862964630127\n",
      "The loss is now 1.1581283807754517\n",
      "The loss is now 1.1483181715011597\n",
      "The loss is now 1.1379446983337402\n",
      "The loss is now 1.129154920578003\n",
      "The loss is now 1.128324270248413\n",
      "The loss is now 1.1332001686096191\n",
      "The loss is now 1.12937331199646\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_5_train.txt\")\n",
    "\n",
    "sequence_length = 20\n",
    "num_output = sequence_length+1\n",
    "num_hidden = 4\n",
    "\n",
    "W = torch.nn.Parameter(torch.rand(num_hidden+1, num_hidden))\n",
    "print(\"Weights input & hidden to hidden => \"+str(W))\n",
    "\n",
    "b = torch.nn.Parameter(torch.rand(1, num_hidden))\n",
    "print(\"Bias to hidden => \"+str(b))\n",
    "\n",
    "V = torch.nn.Parameter(torch.rand(num_hidden, num_output))\n",
    "print(\"Weights hidden to output categories => \"+str(V))\n",
    "\n",
    "d = torch.nn.Parameter(torch.rand(1, num_output))\n",
    "print(\"Bias to output categories => \"+str(d))\n",
    "\n",
    "optimizer = torch.optim.Adam([W, b, V, d], lr=0.01)\n",
    "\n",
    "for j in range(1001):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    \n",
    "    labels, features = data.get_sample(1000)\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(\"Features: \"+str(features))\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(\"Target: \"+str(target))\n",
    "    \n",
    "    state = torch.autograd.Variable(torch.zeros(features.size()[0],num_hidden))\n",
    "    #print(\"State: \"+str(state))\n",
    "\n",
    "    for i in range(sequence_length):\n",
    "        features_at_current_step = torch.unsqueeze(features[:,i], 1)\n",
    "        #print(\"Features at current step: \"+str(features_at_current_step))\n",
    "        x = torch.cat((state, features_at_current_step), 1)\n",
    "        state = F.tanh(x.mm(W) + b)\n",
    "    \n",
    "    result = state.mm(V) + d\n",
    "    \n",
    "    loss = F.cross_entropy(result, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if j % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data[0]))\n",
    "    \n",
    "#print(\"The first layer weights are now \"+str(weights1.data))\n",
    "#print(\"\\tand the second layer's weights are now \"+str(weights2.data))\n",
    "\n",
    "torch.save(W, \"models/toy_problem_5_trained_sequential_deep_model_weights1.bin\")\n",
    "torch.save(b, \"models/toy_problem_5_trained_sequential_deep_model_bias1.bin\")\n",
    "torch.save(V, \"models/toy_problem_5_trained_sequential_deep_model_weights2.bin\")\n",
    "torch.save(d, \"models/toy_problem_5_trained_sequential_deep_model_bias2.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss has been steadily decreasing.  Testing the model would be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
