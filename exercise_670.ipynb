{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Neural Network Classifier for Toy Problem 3\n",
    "\n",
    "In exercise 630, we built a single layer classifier for Toy Problem 3.\n",
    "\n",
    "We saw that for the single-layer neural network, the loss did not decrease despite the prolonged use of gradient descent.\n",
    "\n",
    "The data used in Toy Problem 3 was a version of the XOR function -the categories in this data set are not linearly separable - and so it is impossible for a single-layer neural network (or any other linear classifier) to do well on this dataset.\n",
    "\n",
    "Now let's build a multi-layer classifier for Toy Problem 3 using the ReLU non-linearity (which was discovered/invented only in the year 2000) and see how it fares.\n",
    "\n",
    "We've provided a utility class 'Data' (in data_reader.py) to load the training data (it works for all the toy problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "[1, 1, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "Features:\n",
      "[[-53, 4], [37, -68], [70, 2], [88, -95], [-2, -64], [28, -66], [52, -88], [-29, 7], [-15, 52], [83, -42]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_3_train.txt\")\n",
    "\n",
    "labels, features = data.get_sample()\n",
    "\n",
    "print(\"Labels:\\n\"+str(labels))\n",
    "\n",
    "print(\"Features:\\n\"+str(features))\n",
    "    \n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(\"Labels Tensor:\\n\"+str(target))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(\"Features Tensor:\\n\"+str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights (one set of weights per layer) randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1 => Parameter containing:\n",
      " 0.2741  0.1079  0.5357  0.2768\n",
      " 0.8693  0.0993  0.1424  0.7299\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Weights2 => Parameter containing:\n",
      " 0.2426  0.7439\n",
      " 0.2633  0.9062\n",
      " 0.5629  0.8794\n",
      " 0.1872  0.8052\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "middle = 4\n",
    "\n",
    "weights1 = torch.nn.Parameter(torch.rand(2, middle))\n",
    "print(\"Weights1 => \"+str(weights1))\n",
    "\n",
    "weights2 = torch.nn.Parameter(torch.rand(middle, 2))\n",
    "print(\"Weights2 => \"+str(weights2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform 1000 learning iterations below as many times as we want.\n",
    "\n",
    "Notice that the code for the learning iterations is almost identical to that of exercise Adam but that we've used the SGD optimizer class in Pytorch to nudge the weights in the direction they must go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is now 16.446395874023438\n",
      "The loss is now 8.513433456420898\n",
      "The loss is now 2.8546459674835205\n",
      "The loss is now 0.860838770866394\n",
      "The loss is now 0.7812145352363586\n",
      "The loss is now 0.6594237089157104\n",
      "The loss is now 0.4663209021091461\n",
      "The loss is now 0.41401979327201843\n",
      "The loss is now 0.38516300916671753\n",
      "The loss is now 0.3501630425453186\n",
      "The loss is now 0.3344234228134155\n",
      "The loss is now 0.32852596044540405\n",
      "The loss is now 0.31941601634025574\n",
      "The loss is now 0.3146955370903015\n",
      "The loss is now 0.30019524693489075\n",
      "The loss is now 0.28402167558670044\n",
      "The loss is now 0.3068307936191559\n",
      "The loss is now 0.2830791175365448\n",
      "The loss is now 0.2841053605079651\n",
      "The loss is now 0.2849158048629761\n",
      "The loss is now 0.29306915402412415\n",
      "The loss is now 0.2820267677307129\n",
      "The loss is now 0.2742811441421509\n",
      "The loss is now 0.2826817035675049\n",
      "The loss is now 0.2755924463272095\n",
      "The loss is now 0.26690950989723206\n",
      "The loss is now 0.25641873478889465\n",
      "The loss is now 0.2779945731163025\n",
      "The loss is now 0.2528991401195526\n",
      "The loss is now 0.2694358229637146\n",
      "The loss is now 0.25364363193511963\n",
      "The loss is now 0.2441549152135849\n",
      "The loss is now 0.23712854087352753\n",
      "The loss is now 0.2494214028120041\n",
      "The loss is now 0.2609570324420929\n",
      "The loss is now 0.2720220386981964\n",
      "The loss is now 0.25968652963638306\n",
      "The loss is now 0.2565736770629883\n",
      "The loss is now 0.250777930021286\n",
      "The loss is now 0.23296435177326202\n",
      "The loss is now 0.2569635212421417\n",
      "The loss is now 0.25773948431015015\n",
      "The loss is now 0.2372075617313385\n",
      "The loss is now 0.2528846263885498\n",
      "The loss is now 0.23231267929077148\n",
      "The loss is now 0.23952911794185638\n",
      "The loss is now 0.23940491676330566\n",
      "The loss is now 0.23614022135734558\n",
      "The loss is now 0.26105114817619324\n",
      "The loss is now 0.24713672697544098\n",
      "The loss is now 0.24879057705402374\n",
      "The loss is now 0.244643434882164\n",
      "The loss is now 0.2378375381231308\n",
      "The loss is now 0.26599130034446716\n",
      "The loss is now 0.24284210801124573\n",
      "The loss is now 0.23515202105045319\n",
      "The loss is now 0.2368345856666565\n",
      "The loss is now 0.2452230155467987\n",
      "The loss is now 0.23182368278503418\n",
      "The loss is now 0.22589267790317535\n",
      "The loss is now 0.24439778923988342\n",
      "The loss is now 0.23741869628429413\n",
      "The loss is now 0.2355533093214035\n",
      "The loss is now 0.22377018630504608\n",
      "The loss is now 0.22842834889888763\n",
      "The loss is now 0.22642789781093597\n",
      "The loss is now 0.2524789869785309\n",
      "The loss is now 0.21099309623241425\n",
      "The loss is now 0.21936216950416565\n",
      "The loss is now 0.23716367781162262\n",
      "The loss is now 0.2164108157157898\n",
      "The loss is now 0.22749227285385132\n",
      "The loss is now 0.22752545773983002\n",
      "The loss is now 0.21979521214962006\n",
      "The loss is now 0.2216195911169052\n",
      "The loss is now 0.22920507192611694\n",
      "The loss is now 0.22075442969799042\n",
      "The loss is now 0.22727352380752563\n",
      "The loss is now 0.23666097223758698\n",
      "The loss is now 0.22977837920188904\n",
      "The loss is now 0.22213368117809296\n",
      "The loss is now 0.2186097502708435\n",
      "The loss is now 0.23253148794174194\n",
      "The loss is now 0.2248942106962204\n",
      "The loss is now 0.2330339103937149\n",
      "The loss is now 0.23221983015537262\n",
      "The loss is now 0.24755975604057312\n",
      "The loss is now 0.2516493797302246\n",
      "The loss is now 0.22855140268802643\n",
      "The loss is now 0.21921953558921814\n",
      "The loss is now 0.2088175266981125\n",
      "The loss is now 0.21375350654125214\n",
      "The loss is now 0.21617533266544342\n",
      "The loss is now 0.21179212629795074\n",
      "The loss is now 0.23130835592746735\n",
      "The loss is now 0.21256814897060394\n",
      "The loss is now 0.2161765694618225\n",
      "The loss is now 0.20826789736747742\n",
      "The loss is now 0.2223307192325592\n",
      "The loss is now 0.21538424491882324\n",
      "The loss is now 0.2046251893043518\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([weights1, weights2], lr=0.01)\n",
    "\n",
    "for i in range(1001):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    \n",
    "    labels, features = data.get_sample(1000)\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(\"Features: \"+str(features))\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(\"Target: \"+str(target))\n",
    "    \n",
    "    result = features.mm(weights1)\n",
    "    result1 = F.relu(result)\n",
    "    result2 = result1.mm(weights2)\n",
    "    \n",
    "    loss = F.cross_entropy(result2, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data[0]))\n",
    "\n",
    "torch.save(weights1, \"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "torch.save(weights2, \"models/toy_problem_3_trained_deep_model_weights2.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Loss\n",
    "\n",
    "The loss that is printed at the end of every 10 iterations is now seen to decrease.\n",
    "\n",
    "This tells us that the machine learning algorithm is now learning something.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "We can now print the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first layer weights are now \n",
      " 0.2479  0.4143  0.7116 -0.0562\n",
      " 0.5629 -0.0183  0.4606  0.7253\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "and the second layer's weights are now \n",
      " 0.5984  0.3880\n",
      "-0.0612  1.2307\n",
      " 1.0671  0.3753\n",
      " 0.2092  0.7833\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The first layer weights are now \"+str(weights1.data))\n",
    "print(\"and the second layer's weights are now \"+str(weights2.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Test - Toy Problem 3\n",
    "\n",
    "We have just trained a multilayer classifier for Toy Problem 3.\n",
    "\n",
    "It seems to be learning something (the loss on the training data has decreased till about 0.2).\n",
    "\n",
    "Let us evaluate the performance of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.2479  0.4143  0.7116 -0.0562\n",
      " 0.5629 -0.0183  0.4606  0.7253\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Parameter containing:\n",
      " 0.5984  0.3880\n",
      "-0.0612  1.2307\n",
      " 1.0671  0.3753\n",
      " 0.2092  0.7833\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "data = Data(\"data/toy_problem_3_test.txt\")\n",
    "\n",
    "weights1 = torch.load(\"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "print(weights1)\n",
    "weights2 = torch.load(\"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "print(weights2)\n",
    "\n",
    "labels, features = data.get_all()\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(features)\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(target)\n",
    "\n",
    "result = torch.mm(features, weights1)\n",
    "result1 = F.relu(result)\n",
    "result2 = torch.mm(result1, weights2)\n",
    "#print(result2)\n",
    "\n",
    "maxv, observed = torch.max(result2, 1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    total += 1\n",
    "    #print(str(target.data[i]) + \" \" + str(observed.data[i]))\n",
    "    if target.data[i] == observed.data[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, the accuracy is in the high 90s.\n",
    "\n",
    "This is a good score.\n",
    "\n",
    "It tells us that the multi-layer neural network was able to learn the non-linear XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
