{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for Toy Problem 2\n",
    "\n",
    "Now let's build a classifier for Toy Problem 2.\n",
    "\n",
    "We've provided a utility class 'Data' (in data_reader.py) to load the training data (it works for all the toy problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "Features:\n",
      "[[-205, -44, -99], [-167, -84, -42], [121, 73, 82], [2, -54, 100], [94, -10, 83], [5, -23, -46], [59, 29, -57], [-27, -49, -76], [-131, -98, -100], [76, 27, 37]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_2_train.txt\")\n",
    "\n",
    "labels, features = data.get_sample()\n",
    "\n",
    "print(\"Labels:\\n\"+str(labels))\n",
    "\n",
    "print(\"Features:\\n\"+str(features))\n",
    "    \n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(\"Labels Tensor:\\n\"+str(target))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(\"Features Tensor:\\n\"+str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights randomly.\n",
    "\n",
    "We can now perform 100 learning iterations below as many times as we want.\n",
    "\n",
    "Each learning iteration involves a forward pass and a backward pass.\n",
    "\n",
    "The forward pass involves the computation of the loss from the training data and the current parameters.\n",
    "\n",
    "The backward pass is performed automatically by Pytorch when you call loss.backward().\n",
    "\n",
    "Pytorch calculates all the gradients with respect to the loss.\n",
    "\n",
    "These gradients are stored in each parameter's 'grad' member variable.\n",
    "\n",
    "Notice that the code for the learning iterations is identical to that of exercise 530."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.3584  0.5541\n",
      " 0.1191  0.4134\n",
      " 0.0212  0.0041\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "The loss is now 33.18605041503906\n",
      "The loss is now 0.006583866663277149\n",
      "The loss is now 0.000537065789103508\n",
      "The loss is now 0.14890044927597046\n",
      "The loss is now 4.168130544712767e-05\n",
      "The loss is now 0.0009911548113450408\n",
      "The loss is now 8.514986038208008\n",
      "The loss is now 0.6193047761917114\n",
      "The loss is now 2.188471571931e-13\n",
      "The loss is now 3.45133900642395\n",
      "The loss is now 1.425714685199253e-10\n",
      "\tThe weights are now \n",
      " 1.2388 -0.3263\n",
      "-0.4833  1.0158\n",
      "-0.7238  0.7491\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(torch.rand(3, 2))\n",
    "print(weights)\n",
    "\n",
    "for i in range(101):\n",
    "    labels, features = data.get_sample(10)\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(data)\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(target)\n",
    "    \n",
    "    result = torch.mm(features, weights)\n",
    "    #print(result)\n",
    "    \n",
    "    loss = F.cross_entropy(result, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "    \n",
    "    #result = F.softmax(result)\n",
    "    #result = torch.log(result)\n",
    "    #print(\"Log softmax: \"+str(result))\n",
    "    #loss = F.nll_loss(result, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    gradient = weights.grad\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    weights.data = weights.data - learning_rate * gradient.data\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data[0]))\n",
    "    \n",
    "    weights.grad.data.zero_()\n",
    "\n",
    "print(\"\\tThe weights are now \"+str(weights.data))\n",
    "\n",
    "torch.save(weights, \"models/toy_problem_2_trained_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameters\n",
    "\n",
    "As we know, the final parameters learnt by the algorithm should look something like this\n",
    "\n",
    "$$\\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\\\ 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "or this\n",
    "\n",
    "$$\\begin{bmatrix}2 & 1 \\\\ 1 & 2 \\\\ 1 & 2\\end{bmatrix}$$\n",
    "\n",
    "Basically the weights values at 0,0 and 1,1 and 2,1 in the matrix should be higher than the weights at 1,0 and 0,1 and 2,0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier for Toy Problem 2\n",
    "\n",
    "We have just trained a classifier for Toy Problem 2.\n",
    "\n",
    "You can evaluate the performance of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 1.2388 -0.3263\n",
      "-0.4833  1.0158\n",
      "-0.7238  0.7491\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "Accuracy: 0.979\n"
     ]
    }
   ],
   "source": [
    "data = Data(\"data/toy_problem_2_test.txt\")\n",
    "\n",
    "weights = torch.load(\"models/toy_problem_2_trained_model.bin\")\n",
    "print(weights)\n",
    "\n",
    "labels, features = data.get_all()\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(features)\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(target)\n",
    "\n",
    "result = torch.mm(features, weights)\n",
    "#print(result)\n",
    "\n",
    "maxv, observed = torch.max(result, 1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    total += 1\n",
    "    #print(str(target.data[i]) + \" \" + str(observed.data[i]))\n",
    "    if target.data[i] == observed.data[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
