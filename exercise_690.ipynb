{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Neural Network using Sigmoid & Bias on Toy Problem 3\n",
    "\n",
    "In exercise 670, we built a multi-layer classifier for Toy Problem 3 and used **the ReLU as the activation function**.\n",
    "\n",
    "We saw that the classifier could get to 97% accuracy on Toy Problem 3.\n",
    "\n",
    "In exercise 680, we saw that if we **used a sigmoid** instead of the ReLU, the classifier would not learn anything.\n",
    "\n",
    "Now, we use a sigmoid again, but also use a bias in addition to the weights.\n",
    "\n",
    "We see that the bias gives the classifier more power (it learns the XOR function or Toy Problem 3).\n",
    "\n",
    "We've provided a utility class 'Data' (in data_reader.py) to load the training data (it works for all the toy problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "[0, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
      "Features:\n",
      "[[-92, -88], [-18, -16], [-86, 96], [-7, 8], [-29, -85], [38, -54], [23, 1], [-11, -49], [77, -44], [-12, 16]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_3_train.txt\")\n",
    "\n",
    "labels, features = data.get_sample()\n",
    "\n",
    "print(\"Labels:\\n\"+str(labels))\n",
    "\n",
    "print(\"Features:\\n\"+str(features))\n",
    "    \n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(\"Labels Tensor:\\n\"+str(target))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(\"Features Tensor:\\n\"+str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights and biases (one set of weights and biases per layer) randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1 => Parameter containing:\n",
      " 0.3077  0.7570  0.8574  0.7307\n",
      " 0.1103  0.6138  0.7258  0.7506\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Bias1 => Parameter containing:\n",
      " 0.9492  0.6303  0.3055  0.0644\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "Weights2 => Parameter containing:\n",
      " 0.0210  0.0807\n",
      " 0.9764  0.7614\n",
      " 0.7940  0.0360\n",
      " 0.3574  0.7422\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "Bias2 => Parameter containing:\n",
      " 0.0214  0.4595\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "middle = 4\n",
    "\n",
    "weights1 = torch.nn.Parameter(torch.rand(2, middle))\n",
    "print(\"Weights1 => \"+str(weights1))\n",
    "\n",
    "bias1 = torch.nn.Parameter(torch.rand(1, middle))\n",
    "print(\"Bias1 => \"+str(bias1))\n",
    "\n",
    "weights2 = torch.nn.Parameter(torch.rand(middle, 2))\n",
    "print(\"Weights2 => \"+str(weights2))\n",
    "\n",
    "bias2 = torch.nn.Parameter(torch.rand(1,2))\n",
    "print(\"Bias2 => \"+str(bias2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform 1000 learning iterations below as many times as we want.\n",
    "\n",
    "Notice that the code for the learning iterations is almost identical to that of exercise 630 but that we've used the Adam optimizer class in Pytorch to nudge the weights in the direction they must go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is now 0.7097412943840027\n",
      "The loss is now 0.694894552230835\n",
      "The loss is now 0.6865906119346619\n",
      "The loss is now 0.6885678172111511\n",
      "The loss is now 0.6736515164375305\n",
      "The loss is now 0.6638016104698181\n",
      "The loss is now 0.6568567156791687\n",
      "The loss is now 0.638590395450592\n",
      "The loss is now 0.6303065419197083\n",
      "The loss is now 0.6224498152732849\n",
      "The loss is now 0.6118046045303345\n",
      "The loss is now 0.5993473529815674\n",
      "The loss is now 0.5863654017448425\n",
      "The loss is now 0.5827643871307373\n",
      "The loss is now 0.5612202882766724\n",
      "The loss is now 0.560285210609436\n",
      "The loss is now 0.5578759908676147\n",
      "The loss is now 0.5514324307441711\n",
      "The loss is now 0.5636094808578491\n",
      "The loss is now 0.5348323583602905\n",
      "The loss is now 0.5476241707801819\n",
      "The loss is now 0.5421059727668762\n",
      "The loss is now 0.5163143277168274\n",
      "The loss is now 0.5056638121604919\n",
      "The loss is now 0.48068585991859436\n",
      "The loss is now 0.4549301266670227\n",
      "The loss is now 0.4588942527770996\n",
      "The loss is now 0.4278877377510071\n",
      "The loss is now 0.44272759556770325\n",
      "The loss is now 0.4220033884048462\n",
      "The loss is now 0.4381779432296753\n",
      "The loss is now 0.41663360595703125\n",
      "The loss is now 0.4053451418876648\n",
      "The loss is now 0.42798131704330444\n",
      "The loss is now 0.38908880949020386\n",
      "The loss is now 0.4107622504234314\n",
      "The loss is now 0.43867194652557373\n",
      "The loss is now 0.3896622359752655\n",
      "The loss is now 0.3668322265148163\n",
      "The loss is now 0.4089701175689697\n",
      "The loss is now 0.3593493402004242\n",
      "The loss is now 0.37018275260925293\n",
      "The loss is now 0.3643207848072052\n",
      "The loss is now 0.40598684549331665\n",
      "The loss is now 0.38975197076797485\n",
      "The loss is now 0.375488817691803\n",
      "The loss is now 0.38399362564086914\n",
      "The loss is now 0.35506337881088257\n",
      "The loss is now 0.4377359449863434\n",
      "The loss is now 0.38831281661987305\n",
      "The loss is now 0.3868718147277832\n",
      "The loss is now 0.37083932757377625\n",
      "The loss is now 0.3950256407260895\n",
      "The loss is now 0.3805462419986725\n",
      "The loss is now 0.3891065716743469\n",
      "The loss is now 0.38657402992248535\n",
      "The loss is now 0.36577877402305603\n",
      "The loss is now 0.3878415822982788\n",
      "The loss is now 0.3672167956829071\n",
      "The loss is now 0.38940638303756714\n",
      "The loss is now 0.3801412582397461\n",
      "The loss is now 0.3622797727584839\n",
      "The loss is now 0.37604016065597534\n",
      "The loss is now 0.3866814076900482\n",
      "The loss is now 0.3761291801929474\n",
      "The loss is now 0.39348965883255005\n",
      "The loss is now 0.38733693957328796\n",
      "The loss is now 0.3821299374103546\n",
      "The loss is now 0.38310176134109497\n",
      "The loss is now 0.37598469853401184\n",
      "The loss is now 0.3935747444629669\n",
      "The loss is now 0.43379807472229004\n",
      "The loss is now 0.41562286019325256\n",
      "The loss is now 0.383353054523468\n",
      "The loss is now 0.3506222367286682\n",
      "The loss is now 0.37205177545547485\n",
      "The loss is now 0.36308714747428894\n",
      "The loss is now 0.37434372305870056\n",
      "The loss is now 0.35616323351860046\n",
      "The loss is now 0.36098992824554443\n",
      "The loss is now 0.38327717781066895\n",
      "The loss is now 0.36739951372146606\n",
      "The loss is now 0.38435718417167664\n",
      "The loss is now 0.3600887358188629\n",
      "The loss is now 0.3986724317073822\n",
      "The loss is now 0.4195016026496887\n",
      "The loss is now 0.4064677655696869\n",
      "The loss is now 0.3649853765964508\n",
      "The loss is now 0.3683834373950958\n",
      "The loss is now 0.38439691066741943\n",
      "The loss is now 0.3780195116996765\n",
      "The loss is now 0.361249715089798\n",
      "The loss is now 0.4162551164627075\n",
      "The loss is now 0.38676774501800537\n",
      "The loss is now 0.3976115584373474\n",
      "The loss is now 0.3729167878627777\n",
      "The loss is now 0.38617751002311707\n",
      "The loss is now 0.39360350370407104\n",
      "The loss is now 0.39585593342781067\n",
      "The loss is now 0.38568079471588135\n",
      "The loss is now 0.4044352173805237\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([weights1, weights2, bias1, bias2], lr=0.01)\n",
    "\n",
    "for i in range(1001):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    \n",
    "    labels, features = data.get_sample(1000)\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(\"Features: \"+str(features))\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(\"Target: \"+str(target))\n",
    "    \n",
    "    result = features.mm(weights1) + bias1\n",
    "    result1 = F.sigmoid(result)\n",
    "    result2 = result1.mm(weights2) + bias2\n",
    "    \n",
    "    loss = F.cross_entropy(result2, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data[0]))\n",
    "\n",
    "torch.save(weights1, \"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "torch.save(weights2, \"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "torch.save(bias1, \"models/toy_problem_3_trained_deep_model_bias1.bin\")\n",
    "torch.save(bias2, \"models/toy_problem_3_trained_deep_model_bias2.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Loss\n",
    "\n",
    "Observe the loss that is printed at the end of every 10 iterations.\n",
    "\n",
    "The loss does decrease this time.\n",
    "\n",
    "This tells us that the machine learning algorithm is learning.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "We can now print the weights and the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first layer weights are now \n",
      " 0.0454  0.0642  0.3122  0.2164\n",
      " 0.0561  0.0568  0.2685  0.1900\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "and the second layer's weights are now \n",
      "-2.6972  2.7989\n",
      " 3.8796 -2.1417\n",
      " 1.4229 -0.5929\n",
      "-0.5432  1.6427\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "The first layer bias is now \n",
      " 4.7242 -5.3183 -4.5289  4.4326\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "and the second layer's bias is now \n",
      " 2.3795 -1.8987\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The first layer weights are now \"+str(weights1.data))\n",
    "print(\"and the second layer's weights are now \"+str(weights2.data))\n",
    "print(\"The first layer bias is now \"+str(bias1.data))\n",
    "print(\"and the second layer's bias is now \"+str(bias2.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Test - Toy Problem 3\n",
    "\n",
    "We have just trained a multilayer classifier for Toy Problem 3.\n",
    "\n",
    "It seems to be learning (the loss on the training data decreases).\n",
    "\n",
    "Let us evaluate the performance of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.0454  0.0642  0.3122  0.2164\n",
      " 0.0561  0.0568  0.2685  0.1900\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Parameter containing:\n",
      "-2.6972  2.7989\n",
      " 3.8796 -2.1417\n",
      " 1.4229 -0.5929\n",
      "-0.5432  1.6427\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "Parameter containing:\n",
      " 4.7242 -5.3183 -4.5289  4.4326\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "Parameter containing:\n",
      " 2.3795 -1.8987\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Accuracy: 0.861\n"
     ]
    }
   ],
   "source": [
    "data = Data(\"data/toy_problem_3_test.txt\")\n",
    "\n",
    "weights1 = torch.load(\"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "print(weights1)\n",
    "weights2 = torch.load(\"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "print(weights2)\n",
    "bias1 = torch.load(\"models/toy_problem_3_trained_deep_model_bias1.bin\")\n",
    "print(bias1)\n",
    "bias2 = torch.load(\"models/toy_problem_3_trained_deep_model_bias2.bin\")\n",
    "print(bias2)\n",
    "\n",
    "labels, features = data.get_all()\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(features)\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(target)\n",
    "\n",
    "result = torch.mm(features, weights1) + bias1\n",
    "result1 = F.sigmoid(result)\n",
    "result2 = torch.mm(result1, weights2) + bias2\n",
    "#print(result2)\n",
    "\n",
    "maxv, observed = torch.max(result2, 1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    total += 1\n",
    "    #print(str(target.data[i]) + \" \" + str(observed.data[i]))\n",
    "    if target.data[i] == observed.data[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, the accuracy is better than 50%.\n",
    "\n",
    "The classifier has fared better than before when using bias terms.\n",
    "\n",
    "It tells us that the multi-layer neural network (with a bias term) **is able to learn the non-linear XOR function using the sigmoid activation function**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
