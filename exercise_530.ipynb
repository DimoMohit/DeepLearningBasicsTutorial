{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the Loss over Training Data\n",
    "\n",
    "When you have training data for classification, the loss is calculated based on the performance of the classifier over a batch of the training data.\n",
    "\n",
    "The parameters of the neural network (the weights and biases) which minimize the loss are discovered by descending the loss gradient, as you can see below.\n",
    "\n",
    "We've provided a utility class 'Data' (in data_reader.py) to load the training data (it works for all the toy problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "[0, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "Features:\n",
      "[[-12, -79], [97, 27], [-45, 84], [4, -71], [81, 10], [-35, -36], [-84, 21], [79, -12], [24, 38], [-98, 78]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_1_train.txt\")\n",
    "\n",
    "labels, features = data.get_sample()\n",
    "\n",
    "print(\"Labels:\\n\"+str(labels))\n",
    "\n",
    "print(\"Features:\\n\"+str(features))\n",
    "    \n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(\"Labels Tensor:\\n\"+str(target))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(\"Features Tensor:\\n\"+str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights randomly.\n",
    "\n",
    "We can now perform 100 learning iterations below as many times as we want.\n",
    "\n",
    "Each learning iteration involves a forward pass and a backward pass.\n",
    "\n",
    "The forward pass involves the computation of the loss from the training data and the current parameters.\n",
    "\n",
    "The backward pass is performed automatically by Pytorch when you call loss.backward().\n",
    "\n",
    "Pytorch calculates all the gradients with respect to the loss.\n",
    "\n",
    "These gradients are stored in each parameter's 'grad' member variable.\n",
    "\n",
    "Notice that the code for the learning iterations is almost identical to that of exercise 510."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.6774,  0.9258],\n",
      "        [ 0.7395,  0.8862]])\n",
      "The loss is now 10.548181533813477\n",
      "The loss is now 1.5773188351886347e-05\n",
      "The loss is now 0.0036009151954203844\n",
      "The loss is now 0.010963144712150097\n",
      "The loss is now 0.000298842613119632\n",
      "The loss is now 9.246605969792654e-08\n",
      "The loss is now 0.007362870965152979\n",
      "The loss is now 9.522382242721505e-06\n",
      "The loss is now 5.337378983227836e-09\n",
      "The loss is now 0.1943223923444748\n",
      "The loss is now 0.07547501474618912\n",
      "\tThe weights are now tensor([[ 1.2251,  0.3782],\n",
      "        [ 0.3570,  1.2688]])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(torch.rand(2, 2))\n",
    "\n",
    "print(weights)\n",
    "\n",
    "for i in range(101):\n",
    "    \n",
    "    if weights.grad is not None:\n",
    "        weights.grad.data.zero_()\n",
    "\n",
    "    # Forward pass\n",
    "    \n",
    "    labels, features = data.get_sample()\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(data)\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(target)\n",
    "    \n",
    "    result = torch.mm(features, weights)\n",
    "    #print(result)\n",
    "    \n",
    "    loss = F.cross_entropy(result, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "    \n",
    "    # Backward pass\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    gradient = weights.grad\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    weights.data = weights.data - learning_rate * gradient.data\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data.item()))\n",
    "    \n",
    "print(\"\\tThe weights are now \"+str(weights.data))\n",
    "\n",
    "torch.save(weights, \"models/toy_problem_1_trained_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameters\n",
    "\n",
    "As we know, the final parameters learnt by the algorithm should look something like this\n",
    "\n",
    "$$\\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "or this\n",
    "\n",
    "$$\\begin{bmatrix}2 & 1 \\\\ 1 & 2\\end{bmatrix}$$\n",
    "\n",
    "Basically the weights values at 0,0 and 1,1 in the matrix should be higher than the weights at 1,0 and 0,1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier for Toy Problem 1\n",
    "\n",
    "We have just trained a classifier for Toy Problem 1.\n",
    "\n",
    "You can evaluate the performance of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2251,  0.3782],\n",
      "        [ 0.3570,  1.2688]])\n",
      "Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "data = Data(\"data/toy_problem_1_test.txt\")\n",
    "\n",
    "weights = torch.load(\"models/toy_problem_1_trained_model.bin\")\n",
    "\n",
    "print(weights)\n",
    "\n",
    "labels, features = data.get_all()\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(features)\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(target)\n",
    "\n",
    "result = torch.mm(features, weights)\n",
    "#print(result)\n",
    "\n",
    "maxv, observed = torch.max(result, 1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    total += 1\n",
    "    #print(str(target.data[i]) + \" \" + str(observed.data[i]))\n",
    "    if target.data[i] == observed.data[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
