{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Algorithm\n",
    "\n",
    "We explore backpropagation assuming the features are $\\begin{bmatrix}-10 & 20\\end{bmatrix}$ and the correct category is $1$ which is one-hot encoded as $\\begin{bmatrix}0 & 1\\end{bmatrix}$.\n",
    "\n",
    "The parameters of the neural network (the weights and biases) which minimize the loss are discovered by descending the loss gradient.\n",
    "\n",
    "In exercise 530, we calculated the partial derivative of the loss with respect to each parameter using Pytorch's built-in gradient computating function - the '.backward()' method.\n",
    "\n",
    "In this exercise, we compute those partial derivatives manually, so that we can learn the nuts and bolts of the backpropagation algorithm.\n",
    "\n",
    "The derivations of the equations we use in this exercise can be found in the Powerpoint slides that accompany this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters (weights): \n",
      " 1  2\n",
      " 2  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Parameters (bias): \n",
      " 0  0\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Features: \n",
      "-10  20\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "One hot encoding of the correct class: \n",
      " 0  1\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "weights = torch.nn.Parameter(torch.Tensor([[1,2],[2,1]]))\n",
    "print(\"Parameters (weights): \"+str(weights.data))\n",
    "\n",
    "bias = torch.nn.Parameter(torch.Tensor([[0,0]]))\n",
    "print(\"Parameters (bias): \"+str(bias.data))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor([[-10, 20]]))\n",
    "print(\"Features: \"+str(features.data))\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor([1]))\n",
    "#print(\"The correct class: \"+str(target.data))\n",
    "\n",
    "one_hot_target = torch.autograd.Variable(torch.Tensor([[0, 1]]))\n",
    "print(\"One hot encoding of the correct class: \"+str(one_hot_target.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the weights are $\\begin{bmatrix}1 & 2 \\\\ 2 & 1\\end{bmatrix}$ at start.\n",
    "\n",
    "The training is assumed to proceed one data point at a time (batch size of 1).\n",
    "\n",
    "For this iteration of training, the features are $\\begin{bmatrix}-10 & 20\\end{bmatrix}$ and the correct category is $1$ which is one-hot encoded as $\\begin{bmatrix}0 & 1\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: \n",
      " 30   0\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Softmax of c: \n",
      " 1.0000e+00  9.3576e-14\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Log softmax of c: \n",
      "-9.2371e-14 -3.0000e+01\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "NLL + log_softmax loss: \n",
      " 30\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Cross entropy loss: \n",
      " 30\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "transpose of features: \n",
      "-10\n",
      " 20\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "grad of loss wrt to c: \n",
      " 1 -1\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "grad of loss wrt to weights: \n",
      "-10  10\n",
      " 20 -20\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "grad of loss wrt to bias: \n",
      " 1 -1\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "\tThe manually computed gradient of the loss with respect to weights is \n",
      "-10  10\n",
      " 20 -20\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\tThe manually computed gradient of the loss with respect to bias is \n",
      " 1 -1\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "\tThe weights are now \n",
      " 1.1000  1.9000\n",
      " 1.8000  1.2000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\tThe bias is now \n",
      "1.00000e-03 *\n",
      " -10.0000 10.0000\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if weights.grad is not None:\n",
    "    weights.grad.data.zero_()\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "result = torch.mm(features, weights) + bias\n",
    "print(\"c: \"+str(result.data))\n",
    "\n",
    "softmax_result = F.softmax(result, dim=1)\n",
    "print(\"Softmax of c: \"+str(softmax_result.data))\n",
    "\n",
    "log_softmax_result = F.log_softmax(result, dim=1)\n",
    "print(\"Log softmax of c: \"+str(log_softmax_result.data))\n",
    "\n",
    "loss_nll_softmax = F.nll_loss(log_softmax_result, target)\n",
    "print(\"NLL + log_softmax loss: \"+str(loss_nll_softmax.data))\n",
    "\n",
    "loss = F.cross_entropy(result, target)\n",
    "print(\"Cross entropy loss: \"+str(loss.data))\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "print(\"transpose of features: \"+str(features.data.t()))\n",
    "\n",
    "grad_c = (softmax_result.data - one_hot_target.data)\n",
    "print(\"grad of loss wrt to c: \"+str(grad_c))\n",
    "\n",
    "grad_weights = features.data.t().mm(grad_c)\n",
    "print(\"grad of loss wrt to weights: \"+str(grad_weights))\n",
    "\n",
    "grad_bias = grad_c\n",
    "print(\"grad of loss wrt to bias: \"+str(grad_bias))\n",
    "\n",
    "print(\"\\tThe manually computed gradient of the loss with respect to weights is \"+str(grad_weights))\n",
    "\n",
    "print(\"\\tThe manually computed gradient of the loss with respect to bias is \"+str(grad_bias))\n",
    "\n",
    "# You can now update the weights and bias\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "weights.data = weights.data - learning_rate * grad_weights\n",
    "\n",
    "bias.data = bias.data - learning_rate * grad_bias\n",
    "\n",
    "print(\"\\tThe weights are now \"+str(weights.data))\n",
    "\n",
    "print(\"\\tThe bias is now \"+str(bias.data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameters\n",
    "\n",
    "At the end of the first pass, the weights (after being nudged around) should look something like this\n",
    "\n",
    "$$\\begin{bmatrix}1.1 & 1.9 \\\\ 1.8 & 1.2\\end{bmatrix}$$\n",
    "\n",
    "and the bias should look like this\n",
    "\n",
    "$$\\begin{bmatrix}-0.01 & 0.01\\end{bmatrix}$$\n",
    "\n",
    "You can run the code section above again.\n",
    "\n",
    "At the end of the second pass, the weights should look like this\n",
    "\n",
    "$$\\begin{bmatrix}1.2 & 1.8 \\\\ 1.6 & 1.4\\end{bmatrix}$$\n",
    "\n",
    "and the bias should look like this\n",
    "\n",
    "$$\\begin{bmatrix}-0.02 & 0.02\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Check Results\n",
    "\n",
    "You can check that our computations are correct by verifying the same with Pytorch's automatic backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of loss wrt to weights: \n",
      "-10  10\n",
      " 20 -20\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "grad of loss wrt to bias: \n",
      " 1 -1\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can uncomment the following two lines to check our gradient against the automatically computed gradients\n",
    "\n",
    "loss.backward(retain_graph=True)  # Setting retain_graph=True allows us to call loss.backward repeatedly in a local scope\n",
    "\n",
    "grad_weights = weights.grad.data\n",
    "print(\"grad of loss wrt to weights: \"+str(grad_weights))\n",
    "\n",
    "grad_bias = bias.grad.data\n",
    "print(\"grad of loss wrt to bias: \"+str(grad_bias))\n",
    "\n",
    "if weights.grad is not None:\n",
    "    weights.grad.data.zero_()\n",
    "if bias.grad is not None:\n",
    "    bias.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
