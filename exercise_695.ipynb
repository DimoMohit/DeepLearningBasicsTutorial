{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU & Bias on Toy Problem 3\n",
    "\n",
    "In exercise 690, we saw that if we **used a sigmoid** with bias parameters, the classifier could learn the XOR function.\n",
    "\n",
    "Now, we use a ReLU instead of the sigmoid, with the bias terms in addition to the weights.\n",
    "\n",
    "We expect the bias to give the classifier more power.\n",
    "\n",
    "We've provided a utility class 'Data' (in data_reader.py) to load the training data (it works for all the toy problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "[0, 1, 0, 0, 1, 0, 1, 1, 0, 0]\n",
      "Features:\n",
      "[[48, 81], [-39, 75], [-48, -94], [81, 5], [-5, 50], [-92, -26], [-24, 22], [-10, 94], [-75, -92], [78, 62]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_3_train.txt\")\n",
    "\n",
    "labels, features = data.get_sample()\n",
    "\n",
    "print(\"Labels:\\n\"+str(labels))\n",
    "\n",
    "print(\"Features:\\n\"+str(features))\n",
    "    \n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(\"Labels Tensor:\\n\"+str(target))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(\"Features Tensor:\\n\"+str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights and biases (one set of weights and biases per layer) randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1 => Parameter containing:\n",
      "tensor([[ 0.5236,  0.9937,  0.8793,  0.5058],\n",
      "        [ 0.6404,  0.0323,  0.4522,  0.2071]])\n",
      "Bias1 => Parameter containing:\n",
      "tensor([[ 0.5174,  0.8738,  0.8567,  0.4814]])\n",
      "Weights2 => Parameter containing:\n",
      "tensor([[ 0.4804,  0.7107],\n",
      "        [ 0.9554,  0.6189],\n",
      "        [ 0.0527,  0.9612],\n",
      "        [ 0.8901,  0.3731]])\n",
      "Bias2 => Parameter containing:\n",
      "tensor([[ 0.8208,  0.0828]])\n"
     ]
    }
   ],
   "source": [
    "middle = 4\n",
    "\n",
    "weights1 = torch.nn.Parameter(torch.rand(2, middle))\n",
    "print(\"Weights1 => \"+str(weights1))\n",
    "\n",
    "bias1 = torch.nn.Parameter(torch.rand(1, middle))\n",
    "print(\"Bias1 => \"+str(bias1))\n",
    "\n",
    "weights2 = torch.nn.Parameter(torch.rand(middle, 2))\n",
    "print(\"Weights2 => \"+str(weights2))\n",
    "\n",
    "bias2 = torch.nn.Parameter(torch.rand(1,2))\n",
    "print(\"Bias2 => \"+str(bias2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform 1000 learning iterations below as many times as we want.\n",
    "\n",
    "Notice that the code for the learning iterations is almost identical to that of exercise 630 but that we've used the Adam optimizer class in Pytorch to nudge the weights in the direction they must go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is now 9.415506362915039\n",
      "The loss is now 3.044528007507324\n",
      "The loss is now 1.3942745923995972\n",
      "The loss is now 1.148698091506958\n",
      "The loss is now 0.7427251935005188\n",
      "The loss is now 0.5695450901985168\n",
      "The loss is now 0.5076305270195007\n",
      "The loss is now 0.4617869555950165\n",
      "The loss is now 0.4477054178714752\n",
      "The loss is now 0.42276179790496826\n",
      "The loss is now 0.39894604682922363\n",
      "The loss is now 0.3705463111400604\n",
      "The loss is now 0.33704569935798645\n",
      "The loss is now 0.28844645619392395\n",
      "The loss is now 0.25356411933898926\n",
      "The loss is now 0.24803462624549866\n",
      "The loss is now 0.22278311848640442\n",
      "The loss is now 0.2283671498298645\n",
      "The loss is now 0.21863271296024323\n",
      "The loss is now 0.21036402881145477\n",
      "The loss is now 0.2114981859922409\n",
      "The loss is now 0.1935742348432541\n",
      "The loss is now 0.18528889119625092\n",
      "The loss is now 0.17361214756965637\n",
      "The loss is now 0.18092025816440582\n",
      "The loss is now 0.16048189997673035\n",
      "The loss is now 0.15003038942813873\n",
      "The loss is now 0.15122634172439575\n",
      "The loss is now 0.1110343411564827\n",
      "The loss is now 0.11297444254159927\n",
      "The loss is now 0.09988386183977127\n",
      "The loss is now 0.10233578085899353\n",
      "The loss is now 0.0994972363114357\n",
      "The loss is now 0.10154029726982117\n",
      "The loss is now 0.0944165289402008\n",
      "The loss is now 0.09387800842523575\n",
      "The loss is now 0.0814482718706131\n",
      "The loss is now 0.08385322988033295\n",
      "The loss is now 0.0786072239279747\n",
      "The loss is now 0.07954376190900803\n",
      "The loss is now 0.07135239243507385\n",
      "The loss is now 0.07284146547317505\n",
      "The loss is now 0.08723416179418564\n",
      "The loss is now 0.08779743313789368\n",
      "The loss is now 0.08095252513885498\n",
      "The loss is now 0.07630183547735214\n",
      "The loss is now 0.07944735139608383\n",
      "The loss is now 0.08040589839220047\n",
      "The loss is now 0.09146849811077118\n",
      "The loss is now 0.08216593414545059\n",
      "The loss is now 0.06864748895168304\n",
      "The loss is now 0.08161313086748123\n",
      "The loss is now 0.06790191680192947\n",
      "The loss is now 0.07359132170677185\n",
      "The loss is now 0.06329701095819473\n",
      "The loss is now 0.08169446140527725\n",
      "The loss is now 0.08706579357385635\n",
      "The loss is now 0.06882745772600174\n",
      "The loss is now 0.07928737998008728\n",
      "The loss is now 0.07160840183496475\n",
      "The loss is now 0.06403081119060516\n",
      "The loss is now 0.05752252787351608\n",
      "The loss is now 0.07105891406536102\n",
      "The loss is now 0.07587005198001862\n",
      "The loss is now 0.06596148759126663\n",
      "The loss is now 0.06475716084241867\n",
      "The loss is now 0.06555876135826111\n",
      "The loss is now 0.06934463977813721\n",
      "The loss is now 0.05578296631574631\n",
      "The loss is now 0.07324113696813583\n",
      "The loss is now 0.06405163556337357\n",
      "The loss is now 0.0731983333826065\n",
      "The loss is now 0.07515309751033783\n",
      "The loss is now 0.06706608831882477\n",
      "The loss is now 0.06171602010726929\n",
      "The loss is now 0.06562776863574982\n",
      "The loss is now 0.06145744398236275\n",
      "The loss is now 0.06337834894657135\n",
      "The loss is now 0.07196810096502304\n",
      "The loss is now 0.0731159970164299\n",
      "The loss is now 0.058083076030015945\n",
      "The loss is now 0.05235196650028229\n",
      "The loss is now 0.06221476569771767\n",
      "The loss is now 0.05459243059158325\n",
      "The loss is now 0.06888261437416077\n",
      "The loss is now 0.05300942808389664\n",
      "The loss is now 0.05940181761980057\n",
      "The loss is now 0.051151905208826065\n",
      "The loss is now 0.0537264458835125\n",
      "The loss is now 0.05976571887731552\n",
      "The loss is now 0.051110584288835526\n",
      "The loss is now 0.05698457732796669\n",
      "The loss is now 0.05273403227329254\n",
      "The loss is now 0.05084240809082985\n",
      "The loss is now 0.0587741918861866\n",
      "The loss is now 0.04751112312078476\n",
      "The loss is now 0.0631968080997467\n",
      "The loss is now 0.06772588193416595\n",
      "The loss is now 0.046095579862594604\n",
      "The loss is now 0.053051672875881195\n",
      "The loss is now 0.0524209626019001\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([weights1, weights2, bias1, bias2], lr=0.01)\n",
    "\n",
    "for i in range(1001):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    \n",
    "    labels, features = data.get_sample(1000)\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(\"Features: \"+str(features))\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(\"Target: \"+str(target))\n",
    "    \n",
    "    result = features.mm(weights1) + bias1\n",
    "    result1 = F.relu(result)\n",
    "    result2 = result1.mm(weights2) + bias2\n",
    "    \n",
    "    loss = F.cross_entropy(result2, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data.item()))\n",
    "\n",
    "torch.save(weights1, \"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "torch.save(weights2, \"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "torch.save(bias1, \"models/toy_problem_3_trained_deep_model_bias1.bin\")\n",
    "torch.save(bias2, \"models/toy_problem_3_trained_deep_model_bias2.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Loss\n",
    "\n",
    "Observe the loss that is printed at the end of every 10 iterations.\n",
    "\n",
    "The loss decrease a lot more than it did when we used the sigmoid activation function.\n",
    "\n",
    "The loss has not ceased decreasing after 1000 iterations (you can train it for thousands more iterations and improve it further).\n",
    "\n",
    "## Parameters\n",
    "\n",
    "We can now print the weights and the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first layer weights are now tensor([[-0.1011,  0.9002,  1.0588,  0.5838],\n",
      "        [ 1.0819,  0.3983, -0.0719,  0.4183]])\n",
      "and the second layer's weights are now tensor([[ 0.3297,  0.8613],\n",
      "        [ 1.0011,  0.5731],\n",
      "        [ 0.0570,  0.9569],\n",
      "        [ 1.1003,  0.1630]])\n",
      "The first layer bias is now tensor([[ 0.9092, -0.7460,  0.8659, -1.4666]])\n",
      "and the second layer's bias is now tensor([[ 2.3198, -1.4162]])\n"
     ]
    }
   ],
   "source": [
    "print(\"The first layer weights are now \"+str(weights1.data))\n",
    "print(\"and the second layer's weights are now \"+str(weights2.data))\n",
    "print(\"The first layer bias is now \"+str(bias1.data))\n",
    "print(\"and the second layer's bias is now \"+str(bias2.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Test - Toy Problem 3\n",
    "\n",
    "We have just trained a multilayer classifier for Toy Problem 3.\n",
    "\n",
    "It seems to be learning (the loss on the training data decreases).\n",
    "\n",
    "Let us evaluate the performance of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1011,  0.9002,  1.0588,  0.5838],\n",
      "        [ 1.0819,  0.3983, -0.0719,  0.4183]])\n",
      "tensor([[ 0.3297,  0.8613],\n",
      "        [ 1.0011,  0.5731],\n",
      "        [ 0.0570,  0.9569],\n",
      "        [ 1.1003,  0.1630]])\n",
      "tensor([[ 0.9092, -0.7460,  0.8659, -1.4666]])\n",
      "tensor([[ 2.3198, -1.4162]])\n",
      "Accuracy: 0.986\n"
     ]
    }
   ],
   "source": [
    "data = Data(\"data/toy_problem_3_test.txt\")\n",
    "\n",
    "weights1 = torch.load(\"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "print(weights1)\n",
    "weights2 = torch.load(\"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "print(weights2)\n",
    "bias1 = torch.load(\"models/toy_problem_3_trained_deep_model_bias1.bin\")\n",
    "print(bias1)\n",
    "bias2 = torch.load(\"models/toy_problem_3_trained_deep_model_bias2.bin\")\n",
    "print(bias2)\n",
    "\n",
    "labels, features = data.get_all()\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(features)\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(target)\n",
    "\n",
    "result = torch.mm(features, weights1) + bias1\n",
    "result1 = F.relu(result)\n",
    "result2 = torch.mm(result1, weights2) + bias2\n",
    "#print(result2)\n",
    "\n",
    "maxv, observed = torch.max(result2, 1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    total += 1\n",
    "    #print(str(target.data[i]) + \" \" + str(observed.data[i]))\n",
    "    if target.data[i] == observed.data[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, the accuracy is way better this time.\n",
    "\n",
    "On this problem at least, ReLU seems to trump Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
