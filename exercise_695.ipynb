{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU & Bias on Toy Problem 3\n",
    "\n",
    "In exercise 690, we saw that if we **used a sigmoid** with bias parameters, the classifier could learn the XOR function.\n",
    "\n",
    "Now, we use a ReLU instead of the sigmoid, with the bias terms in addition to the weights.\n",
    "\n",
    "We expect the bias to give the classifier more power.\n",
    "\n",
    "We've provided a utility class 'Data' (in data_reader.py) to load the training data (it works for all the toy problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "[1, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n",
      "Features:\n",
      "[[-58, 10], [56, -8], [78, -43], [53, -57], [-43, -14], [-9, -68], [97, -58], [40, 67], [-90, -40], [-97, 5]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_3_train.txt\")\n",
    "\n",
    "labels, features = data.get_sample()\n",
    "\n",
    "print(\"Labels:\\n\"+str(labels))\n",
    "\n",
    "print(\"Features:\\n\"+str(features))\n",
    "    \n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(\"Labels Tensor:\\n\"+str(target))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(\"Features Tensor:\\n\"+str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights and biases (one set of weights and biases per layer) randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1 => Parameter containing:\n",
      " 0.4443  0.1887  0.1967  0.7059\n",
      " 0.1097  0.0477  0.3916  0.3397\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Bias1 => Parameter containing:\n",
      " 0.5594  0.7939  0.2642  0.9598\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "Weights2 => Parameter containing:\n",
      " 0.4195  0.0154\n",
      " 0.1422  0.0492\n",
      " 0.5783  0.0510\n",
      " 0.7182  0.8030\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "Bias2 => Parameter containing:\n",
      " 0.5795  0.2133\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "middle = 4\n",
    "\n",
    "weights1 = torch.nn.Parameter(torch.rand(2, middle))\n",
    "print(\"Weights1 => \"+str(weights1))\n",
    "\n",
    "bias1 = torch.nn.Parameter(torch.rand(1, middle))\n",
    "print(\"Bias1 => \"+str(bias1))\n",
    "\n",
    "weights2 = torch.nn.Parameter(torch.rand(middle, 2))\n",
    "print(\"Weights2 => \"+str(weights2))\n",
    "\n",
    "bias2 = torch.nn.Parameter(torch.rand(1,2))\n",
    "print(\"Bias2 => \"+str(bias2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform 1000 learning iterations below as many times as we want.\n",
    "\n",
    "Notice that the code for the learning iterations is almost identical to that of exercise 630 but that we've used the Adam optimizer class in Pytorch to nudge the weights in the direction they must go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is now 4.125748157501221\n",
      "The loss is now 0.9919514656066895\n",
      "The loss is now 0.5785403847694397\n",
      "The loss is now 0.43390631675720215\n",
      "The loss is now 0.3487151861190796\n",
      "The loss is now 0.3022889792919159\n",
      "The loss is now 0.2655614912509918\n",
      "The loss is now 0.23335859179496765\n",
      "The loss is now 0.22074243426322937\n",
      "The loss is now 0.19434170424938202\n",
      "The loss is now 0.18351207673549652\n",
      "The loss is now 0.17960983514785767\n",
      "The loss is now 0.16072426736354828\n",
      "The loss is now 0.1588166058063507\n",
      "The loss is now 0.15214304625988007\n",
      "The loss is now 0.14005398750305176\n",
      "The loss is now 0.1309504359960556\n",
      "The loss is now 0.14025351405143738\n",
      "The loss is now 0.12538421154022217\n",
      "The loss is now 0.11924946308135986\n",
      "The loss is now 0.11144011467695236\n",
      "The loss is now 0.12391482293605804\n",
      "The loss is now 0.10285323858261108\n",
      "The loss is now 0.104105643928051\n",
      "The loss is now 0.08798294514417648\n",
      "The loss is now 0.10140535980463028\n",
      "The loss is now 0.08999021351337433\n",
      "The loss is now 0.09262876957654953\n",
      "The loss is now 0.10037229210138321\n",
      "The loss is now 0.0894279032945633\n",
      "The loss is now 0.09352465718984604\n",
      "The loss is now 0.0922810360789299\n",
      "The loss is now 0.0954316109418869\n",
      "The loss is now 0.09768335521221161\n",
      "The loss is now 0.08437670022249222\n",
      "The loss is now 0.08637005090713501\n",
      "The loss is now 0.08593659847974777\n",
      "The loss is now 0.08954481035470963\n",
      "The loss is now 0.08937779814004898\n",
      "The loss is now 0.07794833928346634\n",
      "The loss is now 0.08690651506185532\n",
      "The loss is now 0.06915730237960815\n",
      "The loss is now 0.08678457140922546\n",
      "The loss is now 0.07131774723529816\n",
      "The loss is now 0.06901682913303375\n",
      "The loss is now 0.06840157508850098\n",
      "The loss is now 0.07630117237567902\n",
      "The loss is now 0.06651302427053452\n",
      "The loss is now 0.06292502582073212\n",
      "The loss is now 0.05767272785305977\n",
      "The loss is now 0.06784097105264664\n",
      "The loss is now 0.066806860268116\n",
      "The loss is now 0.06749556213617325\n",
      "The loss is now 0.0665343701839447\n",
      "The loss is now 0.061629097908735275\n",
      "The loss is now 0.06583021581172943\n",
      "The loss is now 0.05542152002453804\n",
      "The loss is now 0.06301405280828476\n",
      "The loss is now 0.04857783019542694\n",
      "The loss is now 0.05421072989702225\n",
      "The loss is now 0.058813635259866714\n",
      "The loss is now 0.058543406426906586\n",
      "The loss is now 0.051853105425834656\n",
      "The loss is now 0.05481581762433052\n",
      "The loss is now 0.059095367789268494\n",
      "The loss is now 0.06295307725667953\n",
      "The loss is now 0.05653472989797592\n",
      "The loss is now 0.0528193861246109\n",
      "The loss is now 0.05125901475548744\n",
      "The loss is now 0.05975154787302017\n",
      "The loss is now 0.05042266100645065\n",
      "The loss is now 0.04790887609124184\n",
      "The loss is now 0.0477210134267807\n",
      "The loss is now 0.04554154723882675\n",
      "The loss is now 0.04325040057301521\n",
      "The loss is now 0.03838181123137474\n",
      "The loss is now 0.04415174946188927\n",
      "The loss is now 0.05023125186562538\n",
      "The loss is now 0.05075995624065399\n",
      "The loss is now 0.04597511515021324\n",
      "The loss is now 0.04484613984823227\n",
      "The loss is now 0.04202129691839218\n",
      "The loss is now 0.04913634806871414\n",
      "The loss is now 0.05218351632356644\n",
      "The loss is now 0.04435934126377106\n",
      "The loss is now 0.0438697412610054\n",
      "The loss is now 0.055727746337652206\n",
      "The loss is now 0.0406000092625618\n",
      "The loss is now 0.04354415833950043\n",
      "The loss is now 0.04485267400741577\n",
      "The loss is now 0.04633237048983574\n",
      "The loss is now 0.039815112948417664\n",
      "The loss is now 0.0471549890935421\n",
      "The loss is now 0.04505250230431557\n",
      "The loss is now 0.044288136065006256\n",
      "The loss is now 0.03608886897563934\n",
      "The loss is now 0.036651115864515305\n",
      "The loss is now 0.040796976536512375\n",
      "The loss is now 0.04309804365038872\n",
      "The loss is now 0.04087848216295242\n",
      "The loss is now 0.03656338155269623\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([weights1, weights2, bias1, bias2], lr=0.01)\n",
    "\n",
    "for i in range(1001):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    \n",
    "    labels, features = data.get_sample(1000)\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(\"Features: \"+str(features))\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(\"Target: \"+str(target))\n",
    "    \n",
    "    result = features.mm(weights1) + bias1\n",
    "    result1 = F.relu(result)\n",
    "    result2 = result1.mm(weights2) + bias2\n",
    "    \n",
    "    loss = F.cross_entropy(result2, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data[0]))\n",
    "\n",
    "torch.save(weights1, \"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "torch.save(weights2, \"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "torch.save(bias1, \"models/toy_problem_3_trained_deep_model_bias1.bin\")\n",
    "torch.save(bias2, \"models/toy_problem_3_trained_deep_model_bias2.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Loss\n",
    "\n",
    "Observe the loss that is printed at the end of every 10 iterations.\n",
    "\n",
    "The loss decrease a lot more than it did when we used the sigmoid activation function.\n",
    "\n",
    "The loss has not ceased decreasing after 1000 iterations (you can train it for thousands more iterations and improve it further).\n",
    "\n",
    "## Parameters\n",
    "\n",
    "We can now print the weights and the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first layer weights are now \n",
      " 0.4739 -0.0791  0.5436  1.2241\n",
      " 0.7023  0.9879  0.4664 -0.0864\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "and the second layer's weights are now \n",
      " 0.6246 -0.1897\n",
      "-0.4027  0.5942\n",
      " 0.7463 -0.1171\n",
      " 0.3969  1.1243\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "The first layer bias is now \n",
      "-1.3011  0.5464 -1.2566 -0.0181\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "and the second layer's bias is now \n",
      " 2.2892 -1.4964\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The first layer weights are now \"+str(weights1.data))\n",
    "print(\"and the second layer's weights are now \"+str(weights2.data))\n",
    "print(\"The first layer bias is now \"+str(bias1.data))\n",
    "print(\"and the second layer's bias is now \"+str(bias2.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Test - Toy Problem 3\n",
    "\n",
    "We have just trained a multilayer classifier for Toy Problem 3.\n",
    "\n",
    "It seems to be learning (the loss on the training data decreases).\n",
    "\n",
    "Let us evaluate the performance of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.4739 -0.0791  0.5436  1.2241\n",
      " 0.7023  0.9879  0.4664 -0.0864\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Parameter containing:\n",
      " 0.6246 -0.1897\n",
      "-0.4027  0.5942\n",
      " 0.7463 -0.1171\n",
      " 0.3969  1.1243\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "Parameter containing:\n",
      "-1.3011  0.5464 -1.2566 -0.0181\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "Parameter containing:\n",
      " 2.2892 -1.4964\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "data = Data(\"data/toy_problem_3_test.txt\")\n",
    "\n",
    "weights1 = torch.load(\"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "print(weights1)\n",
    "weights2 = torch.load(\"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "print(weights2)\n",
    "bias1 = torch.load(\"models/toy_problem_3_trained_deep_model_bias1.bin\")\n",
    "print(bias1)\n",
    "bias2 = torch.load(\"models/toy_problem_3_trained_deep_model_bias2.bin\")\n",
    "print(bias2)\n",
    "\n",
    "labels, features = data.get_all()\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(features)\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(target)\n",
    "\n",
    "result = torch.mm(features, weights1) + bias1\n",
    "result1 = F.relu(result)\n",
    "result2 = torch.mm(result1, weights2) + bias2\n",
    "#print(result2)\n",
    "\n",
    "maxv, observed = torch.max(result2, 1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    total += 1\n",
    "    #print(str(target.data[i]) + \" \" + str(observed.data[i]))\n",
    "    if target.data[i] == observed.data[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, the accuracy is way better this time.\n",
    "\n",
    "On this problem at least, ReLU seems to trump Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
