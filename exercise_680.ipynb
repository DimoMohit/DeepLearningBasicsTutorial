{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Neural Network using Sigmoid Activation for Toy Problem 3\n",
    "\n",
    "In exercise 670, we built a multi-layer classifier for Toy Problem 3 and used **the ReLU as the activation function**.\n",
    "\n",
    "Let's see **what happens if we use a sigmoid** instead of the ReLU.\n",
    "\n",
    "Note: The sigmoid non-linearity was the most commonly used \"squashing function\" or non-linearity before the advent of the ReLU.\n",
    "\n",
    "Also Note:  We've changed nothing from the code of exercise_670 except the ReLU to the sigmoid.\n",
    "\n",
    "We've provided a utility class 'Data' (in data_reader.py) to load the training data (it works for all the toy problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "[0, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "Features:\n",
      "[[-19, -99], [-35, 47], [59, -70], [-85, -69], [18, -3], [-16, -42], [-23, 45], [66, 9], [-85, -45], [-57, -61]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_reader import Data\n",
    "\n",
    "data = Data(\"data/toy_problem_3_train.txt\")\n",
    "\n",
    "labels, features = data.get_sample()\n",
    "\n",
    "print(\"Labels:\\n\"+str(labels))\n",
    "\n",
    "print(\"Features:\\n\"+str(features))\n",
    "    \n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(\"Labels Tensor:\\n\"+str(target))\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(\"Features Tensor:\\n\"+str(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the weights (one set of weights per layer) randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights1 => Parameter containing:\n",
      " 0.5830  0.2759  0.9354  0.8574\n",
      " 0.5529  0.3738  0.4447  0.3815\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Weights2 => Parameter containing:\n",
      " 0.0206  0.9780\n",
      " 0.7147  0.8743\n",
      " 0.7247  0.8200\n",
      " 0.7998  0.6264\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "middle = 4\n",
    "\n",
    "weights1 = torch.nn.Parameter(torch.rand(2, middle))\n",
    "print(\"Weights1 => \"+str(weights1))\n",
    "\n",
    "weights2 = torch.nn.Parameter(torch.rand(middle, 2))\n",
    "print(\"Weights2 => \"+str(weights2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform 1000 learning iterations below as many times as we want.\n",
    "\n",
    "Notice that the code for the learning iterations is almost identical to that of exercise 630 but that we've used the Adam optimizer class in Pytorch to nudge the weights in the direction they must go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is now 0.7401268482208252\n",
      "The loss is now 0.6937983632087708\n",
      "The loss is now 0.6957014799118042\n",
      "The loss is now 0.6931490302085876\n",
      "The loss is now 0.6924446821212769\n",
      "The loss is now 0.6924837827682495\n",
      "The loss is now 0.690542459487915\n",
      "The loss is now 0.6924556493759155\n",
      "The loss is now 0.6921226382255554\n",
      "The loss is now 0.6910936236381531\n",
      "The loss is now 0.69541996717453\n",
      "The loss is now 0.6904984712600708\n",
      "The loss is now 0.6919944882392883\n",
      "The loss is now 0.6886522173881531\n",
      "The loss is now 0.6914364695549011\n",
      "The loss is now 0.6887767314910889\n",
      "The loss is now 0.6889198422431946\n",
      "The loss is now 0.6891558170318604\n",
      "The loss is now 0.6901046633720398\n",
      "The loss is now 0.6916608810424805\n",
      "The loss is now 0.6940774917602539\n",
      "The loss is now 0.6919639110565186\n",
      "The loss is now 0.6932433843612671\n",
      "The loss is now 0.6892197132110596\n",
      "The loss is now 0.6898062825202942\n",
      "The loss is now 0.6901471018791199\n",
      "The loss is now 0.6912344098091125\n",
      "The loss is now 0.6914899349212646\n",
      "The loss is now 0.6921151280403137\n",
      "The loss is now 0.6930907368659973\n",
      "The loss is now 0.6884926557540894\n",
      "The loss is now 0.695004403591156\n",
      "The loss is now 0.6919713616371155\n",
      "The loss is now 0.6948171854019165\n",
      "The loss is now 0.6922138929367065\n",
      "The loss is now 0.6934698820114136\n",
      "The loss is now 0.6917480230331421\n",
      "The loss is now 0.6914342641830444\n",
      "The loss is now 0.6912474036216736\n",
      "The loss is now 0.693921685218811\n",
      "The loss is now 0.6919436454772949\n",
      "The loss is now 0.6896664500236511\n",
      "The loss is now 0.6920514106750488\n",
      "The loss is now 0.6930276155471802\n",
      "The loss is now 0.6925925612449646\n",
      "The loss is now 0.6917095184326172\n",
      "The loss is now 0.6892465949058533\n",
      "The loss is now 0.694389820098877\n",
      "The loss is now 0.6916676163673401\n",
      "The loss is now 0.6993018388748169\n",
      "The loss is now 0.6947147846221924\n",
      "The loss is now 0.6922687888145447\n",
      "The loss is now 0.6918907761573792\n",
      "The loss is now 0.6912087798118591\n",
      "The loss is now 0.6934152245521545\n",
      "The loss is now 0.6914376616477966\n",
      "The loss is now 0.6920532584190369\n",
      "The loss is now 0.6920921206474304\n",
      "The loss is now 0.6897920370101929\n",
      "The loss is now 0.6900249123573303\n",
      "The loss is now 0.6896593570709229\n",
      "The loss is now 0.6937560439109802\n",
      "The loss is now 0.692868173122406\n",
      "The loss is now 0.6906919479370117\n",
      "The loss is now 0.6918944120407104\n",
      "The loss is now 0.6928110718727112\n",
      "The loss is now 0.6911907196044922\n",
      "The loss is now 0.6898638010025024\n",
      "The loss is now 0.6919572949409485\n",
      "The loss is now 0.6892844438552856\n",
      "The loss is now 0.6921912431716919\n",
      "The loss is now 0.6898066997528076\n",
      "The loss is now 0.6898887157440186\n",
      "The loss is now 0.6895323991775513\n",
      "The loss is now 0.697387158870697\n",
      "The loss is now 0.6905495524406433\n",
      "The loss is now 0.6924980878829956\n",
      "The loss is now 0.6894192099571228\n",
      "The loss is now 0.6905979514122009\n",
      "The loss is now 0.6906936764717102\n",
      "The loss is now 0.6914059519767761\n",
      "The loss is now 0.6932801604270935\n",
      "The loss is now 0.6953914761543274\n",
      "The loss is now 0.6901982426643372\n",
      "The loss is now 0.6880090236663818\n",
      "The loss is now 0.6893234252929688\n",
      "The loss is now 0.6920931339263916\n",
      "The loss is now 0.6921132206916809\n",
      "The loss is now 0.692432165145874\n",
      "The loss is now 0.6920565962791443\n",
      "The loss is now 0.6875384449958801\n",
      "The loss is now 0.6917128562927246\n",
      "The loss is now 0.6930012106895447\n",
      "The loss is now 0.6923585534095764\n",
      "The loss is now 0.6928553581237793\n",
      "The loss is now 0.6886475086212158\n",
      "The loss is now 0.6952103972434998\n",
      "The loss is now 0.6915619373321533\n",
      "The loss is now 0.6912485361099243\n",
      "The loss is now 0.6888242959976196\n",
      "The loss is now 0.6887137293815613\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([weights1, weights2], lr=0.01)\n",
    "\n",
    "for i in range(1001):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    \n",
    "    labels, features = data.get_sample(1000)\n",
    "    \n",
    "    features = torch.autograd.Variable(torch.Tensor(features))\n",
    "    #print(\"Features: \"+str(features))\n",
    "    \n",
    "    target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "    #print(\"Target: \"+str(target))\n",
    "    \n",
    "    result = features.mm(weights1)\n",
    "    result1 = F.sigmoid(result)\n",
    "    result2 = result1.mm(weights2)\n",
    "    \n",
    "    loss = F.cross_entropy(result2, target)\n",
    "    #print(\"Cross entropy loss: \"+str(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"The loss is now \"+str(loss.data[0]))\n",
    "\n",
    "torch.save(weights1, \"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "torch.save(weights2, \"models/toy_problem_3_trained_deep_model_weights2.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Loss\n",
    "\n",
    "Observe the loss that is printed at the end of every 10 iterations.\n",
    "\n",
    "Now matter how many hundreds of times you run the hill-climbing code, the loss does not decrease very much.\n",
    "\n",
    "This tells us that the machine learning algorithm is probably not learning anthing much.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "We can now print the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first layer weights are now \n",
      " 0.6968  0.7275  0.8728  0.8050\n",
      " 0.5757  0.2892  0.7656  0.5792\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "and the second layer's weights are now \n",
      " 0.1540  0.8446\n",
      " 0.7007  0.8883\n",
      " 0.9043  0.6404\n",
      " 0.9424  0.4838\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The first layer weights are now \"+str(weights1.data))\n",
    "print(\"and the second layer's weights are now \"+str(weights2.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Test - Toy Problem 3\n",
    "\n",
    "We have just trained a multilayer classifier for Toy Problem 3.\n",
    "\n",
    "It doesn't seem to be learning anything (the loss on the training data does not decrease).\n",
    "\n",
    "But, to make sure, let us evaluate the performance of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.6968  0.7275  0.8728  0.8050\n",
      " 0.5757  0.2892  0.7656  0.5792\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1540  0.8446\n",
      " 0.7007  0.8883\n",
      " 0.9043  0.6404\n",
      " 0.9424  0.4838\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "Accuracy: 0.465\n"
     ]
    }
   ],
   "source": [
    "data = Data(\"data/toy_problem_3_test.txt\")\n",
    "\n",
    "weights1 = torch.load(\"models/toy_problem_3_trained_deep_model_weights1.bin\")\n",
    "print(weights1)\n",
    "weights2 = torch.load(\"models/toy_problem_3_trained_deep_model_weights2.bin\")\n",
    "print(weights2)\n",
    "\n",
    "labels, features = data.get_all()\n",
    "\n",
    "features = torch.autograd.Variable(torch.Tensor(features))\n",
    "#print(features)\n",
    "\n",
    "target = torch.autograd.Variable(torch.LongTensor(labels))\n",
    "#print(target)\n",
    "\n",
    "result = torch.mm(features, weights1)\n",
    "result1 = F.sigmoid(result)\n",
    "result2 = torch.mm(result1, weights2)\n",
    "#print(result2)\n",
    "\n",
    "maxv, observed = torch.max(result2, 1)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(labels)):\n",
    "    total += 1\n",
    "    #print(str(target.data[i]) + \" \" + str(observed.data[i]))\n",
    "    if target.data[i] == observed.data[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, the accuracy is around 50%.\n",
    "\n",
    "This the classifier hasn't learnt anything at all.\n",
    "\n",
    "It tells us that the multi-layer neural network (without a bias term) was **not able to learn the non-linear XOR function using the sigmoid activation function**, though **it was able to learn the same function using the ReLU** activation function.\n",
    "\n",
    "Note:  This does not mean that a multi-layer neural network (using sigmoid activation) can never learn the non-linear XOR function.  It can, as we shall see in the next exercise - if the neural network uses bias parameters in each layer in addition to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
